{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gemini Enterprise Data Science Test Drive\n",
    "Welcome to the Machine Learning part of the Gemini Enterprise Data Science Test Drive.\n",
    "\n",
    "This jupyter note book will walk you through some exploration of Gemini's ZeroCopy data availability, and how to leverage this function to enable machine learning in Tensorflow\n",
    "\n",
    "We have pre-configured some data sources as part of this demo that provide time series climate data. Namely, we have measurements of sea ice at the north and south poles, atmospheric carbon measurements from the NOAA observatory at Mauna Loa, and global average and max land temperatures.\n",
    "\n",
    "This demo will walk through leveraging these data sources, made available via Gemini Enterprise and it's Zero-Copy Data Virtualozation technology, and how they can be used to create some machine learning models using Tensorflow/Keras so that we can make some predictions about the future of climate change. \n",
    "\n",
    "## Usage\n",
    "\n",
    "This is an interactive so-called notebook with multiple cells that contain necessary code (indicated **In [ ]**) and after running the code, output will be shown in a new cell, indicated with **Out [ ]**.\n",
    "To get started, click the first **In [ ]** cell and press the play icon in the notebook toolar to run the code. Continue running all code cells one by one.\n",
    "If you prefer to pre-run all code cells, click **Run** -> **Restart Kernel and Run All Cells** from the toolbar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Prepare\n",
    "\n",
    "Before we begin, we must install jaydebeapi Python package for connecting to zerocopy - testing in progress to include in Jupyter service\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --user JPype1==0.6.3\n",
    "!pip install --user jaydebeapi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Important:** Please restart the kernel after the install below before proceding. Click **Kernel** -> **Restart Kernel and Clear All Outputs** from the toolbar.\n",
    "\n",
    "Next we import the other pre-requisite packages necessary for our analysis:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import pathlib\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import datetime as dt\n",
    "from keras import initializers\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model,Sequential\n",
    "from keras import optimizers\n",
    "from keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.cluster import KMeans\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.metrics import r2_score\n",
    "import jaydebeapi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** The cell above may produce a warning message which you can ignore.\n",
    "\n",
    "## Step 2: Establish connection to Zero-Copy Data Virtualization\n",
    "Using the jaydebeapi Python package, we're going to define a resource that uses a JDBC driver to connect to the Zero-Copy service running in Gemini Enterprise cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = jaydebeapi.connect(\"org.apache.calcite.avatica.remote.Driver\",\n",
    "\"jdbc:avatica:remote:url=http://zero-copyadapter.marathon.l4lb.thisdcos.directory:8765;serialization=JSON\",\n",
    "[\"admin\",\"admin\"],\n",
    "\"/mnt/mesos/sandbox/avatica-1.13.0.jar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've configured our connection, let's test out sending it a SQL query so we can see what's returned. CLIMATE.NORTHSEAICE is a table defined in our model.json file as part of our ZeroCopy configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curs = conn.cursor()\n",
    "curs.execute(\"select * from CLIMATE.NORTHSEAICE limit 3\")\n",
    "curs.fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Explore and prepare data\n",
    "Great! We know SQL is working so now we can start moving forward with harvesting data from ZeroCopy for analysis. Below is a more complex query that will join several of the ZeroCopy data sources so we have northern and southern sea ice extent, along with land temperatures in a single output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "climatedata_sql_str = '''select NS.\"Year\", NS.\"Month\", NS.\"Day\", NS.\"Extent\" as \"ExtentNorth\", SS.\"Extent\" as \"ExtentSouth\", GT.\"LandAverageTemperature\", GT.\"LandMaxTemperature\"\n",
    "  FROM CLIMATE.NORTHSEAICE NS\n",
    "  JOIN CLIMATE.SOUTHSEAICE SS ON NS.\"Year\"=SS.\"Year\" \n",
    "      AND NS.\"Month\"=SS.\"Month\"\n",
    "      AND NS.\"Day\"=SS.\"Day\"\n",
    "  LEFT JOIN (SELECT SUBSTRING(GT.\"dt\", 1, 4) AS \"Year\", SUBSTRING(GT.\"dt\", 6, 2) AS \"Month\", SUBSTRING(GT.\"dt\", 9, 2) AS \"Day\", \n",
    "  \"LandAverageTemperature\", \"LandMaxTemperature\" FROM CLIMATE.GLOBALTEMPERATURES GT) GT ON NS.\"Year\"=GT.\"Year\"\n",
    "      AND NS.\"Month\"=GT.\"Month\"\n",
    "      AND NS.\"Day\"=GT.\"Day\"'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the query configured, we can now send this query to ZeroCopy and read the output straight into a Pandas Dataframe, whhere we can manipulate and massage the data further, if necessary before we start with our modeling tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "climate_sql_query = pd.read_sql_query(climatedata_sql_str, conn)\n",
    "climate_df = pd.DataFrame(climate_sql_query)\n",
    "print(climate_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "curs = conn.cursor()\n",
    "curs.execute(\"select * from CLIMATE.C02MAUNALOA limit 3;\")\n",
    "curs.fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, our data table has date spread across three columns, one each for day, month, and year. Let's modify our climate_df dataframe to have a single date field. We also need to drop the first row in our climate_df dataframe, which seems to be a header from the original CSV. We should also convery types where necessary, and handle our duplicate columns, as the fields defined as 'NorthExtent' and 'SouthExtent' in the SQL query were not picked up with correct headers here in the dataframe. Let's check our datatypes before we proceed with massaging the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "climate_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "climate_df = climate_df.drop([0],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "climate_df['Year_Month_Day'] = climate_df['Year']+\"-\"+climate_df['Month']+\"-\"+climate_df['Day']\n",
    "climate_df['Year_Month_Day'] = pd.to_datetime(climate_df['Year_Month_Day'])\n",
    "climate_df['Extent'] = climate_df['Extent'].astype('float64')\n",
    "climate_df['LandAverageTemperature'] = climate_df['LandAverageTemperature'].astype('float64')\n",
    "climate_df['LandMaxTemperature'] = climate_df['LandMaxTemperature'].astype('float64')\n",
    "climate_df.columns = ['Year','Month','Day','ExtentNorth','ExtentSouth','LandAverageTemperature','LandMaxTemperature','Year_Month_Day']\n",
    "print(climate_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'd like to get some atmospheric carbon measurement data as well. Let's set up another query for that and pull it in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carbon_sql_str = '''select * from CARBONPPM.\"carbonppm_maunaloa\"'''\n",
    "carbon_sql_query = pd.read_sql_query(carbon_sql_str, conn)\n",
    "carbon_df = pd.DataFrame(carbon_sql_query)\n",
    "print(carbon_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we modify the dataframe, to include a date column, convert types where necessary, and remove ' characters and convert negative values to nulls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carbon_df['ppm'] = carbon_df[\"'ppm'\"].apply(lambda ppm: ppm.replace(\"'\", \"\"))\n",
    "carbon_df['YYYY'] = carbon_df[\"'YYYY'\"].apply(lambda ppm: ppm.replace(\"'\", \"\"))\n",
    "carbon_df['M'] = carbon_df[\"'M'\"].apply(lambda ppm: ppm.replace(\"'\", \"\"))\n",
    "carbon_df['DD'] = carbon_df[\"'DD'\"].apply(lambda ppm: ppm.replace(\"'\", \"\"))\n",
    "carbon_df = carbon_df.drop([\"'ppm'\",\"'YYYY'\",\"'M'\",\"'DD'\"],axis=1)\n",
    "carbon_df['Year_Month_Day'] = carbon_df['YYYY']+\"-\"+carbon_df['M']+\"-\"+carbon_df['DD']\n",
    "carbon_df['Year_Month_Day'] = pd.to_datetime(carbon_df['Year_Month_Day'])\n",
    "carbon_df['ppm'] = carbon_df['ppm'].astype('float64')\n",
    "carbon_df['ppm'].loc[carbon_df['ppm']==-999.990] = np.nan\n",
    "carbon_df = carbon_df.drop(['YYYY','M','DD'],axis=1)\n",
    "print(carbon_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's merge our two dataframes into a single dataset that we can explore a bit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join operation to find out same records \n",
    "dataset = pd.merge(climate_df,carbon_df,on='Year_Month_Day',how='inner')\n",
    "#dataset.loc[dataset['ppm']==-999.99] = np.nan\n",
    "#dataset = dataset[dataset['ppm'] > 0]\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.drop(['Year','Month','Day'],axis=1)\n",
    "dataset.columns = ['ExtentNorth','ExtentSouth','LandAverageTemperature','LandMaxTemperature','Year_Month_Day','PPM']\n",
    "dataset.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting is a great way of exploring the dataset. Lets take a look at some plots of northern and southern sea ice extent as a function of atmospheric carbon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the results\n",
    "dataset.plot(kind='scatter',x='PPM',y='ExtentNorth',color='blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the results\n",
    "dataset.plot(kind='scatter',x='PPM',y='ExtentSouth',color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's normalize this data. This means converting all the data to a common scale, without distorting the differences in the range of values. After we normalize all the columns, we will replicate the above plot of northern sea ice as a function of atmospheric carbon. Note how the scales on the axes have changed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalizing data set\n",
    "#dataset.PPM = dataset.PPM.astype(float)\n",
    "#dataset.ExtentNorth = dataset.ExtentNorth.astype(float)\n",
    "dataset.PPM = dataset['PPM']/dataset['PPM'].max()\n",
    "dataset.ExtentNorth = dataset['ExtentNorth']/dataset['ExtentNorth'].max()\n",
    "dataset.ExtentSouth = dataset['ExtentSouth']/dataset['ExtentSouth'].max()\n",
    "dataset.LandAverageTemperature = dataset['LandAverageTemperature']/dataset['LandAverageTemperature'].max()\n",
    "dataset.LandMaxTemperature = dataset['LandMaxTemperature']/dataset['LandMaxTemperature'].max()\n",
    "plt.plot(dataset.PPM,dataset.ExtentNorth,'g.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next lets do a K-means cluster analysis. K-means cluster analysis means seperating the data points into a number of clusters, K, where each data point belongs to the cluster with the nearest mean. In this case K=5. Note we must drop nulls first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop nulls\n",
    "dataset = dataset.dropna()\n",
    "#K-means cluster analysis on the data set\n",
    "X = np.array(list(zip(dataset.PPM,dataset.ExtentNorth)))\n",
    "#print(X)\n",
    "kmeans = KMeans(n_clusters=5)\n",
    "kmeans.fit(X)\n",
    "y_kmeans = kmeans.predict(X)\n",
    "plt.scatter(X[:,0], X[:,1], c=y_kmeans, s=50, cmap='viridis')\n",
    "centers = kmeans.cluster_centers_\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also explore our dataset with a few additional plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.plot(kind='scatter',x='PPM',y='ExtentSouth',color='green')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.plot(kind='scatter',x='PPM',y='LandAverageTemperature',color='blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.plot(kind='scatter',x='PPM',y='LandMaxTemperature',color='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's return to our original carbon output. Again, we will explore this, plotting atmostpheric carbon as a function of time. Then we'll perform the same normalization operation. Finally, we'll use Tensorflow/Keras to fit a sequential non-linear regression model to the data that we can use to predict future carbon levels by time. We'll do this in a new temporary dataframe to preserve the original:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carbon_df = carbon_df.sort_values(by=['Year_Month_Day'])\n",
    "carbon_df = carbon_df[carbon_df.ppm > 0]\n",
    "carbon_df.plot(kind='line',x='Year_Month_Day',y='ppm',color='green')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalizing the data set\n",
    "modified_carbon_data = carbon_df #temp df\n",
    "modified_carbon_data['Date_Num'] = modified_carbon_data['Year_Month_Day'].map(dt.datetime.toordinal)\n",
    "modified_carbon_data = modified_carbon_data.drop(['Year_Month_Day'],axis=1)\n",
    "modified_carbon_data = modified_carbon_data.apply(lambda x:x/x.max(),axis=0)\n",
    "print(modified_carbon_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Build model\n",
    "Now we can proceed with using Tensorflow/Keras to build our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keras sequential model backed by tensorflow\n",
    "model = Sequential()\n",
    "model.add(Dense(20, activation='tanh', input_dim=1, kernel_initializer=\"normal\"))\n",
    "model.add(Dense(1, activation='elu', kernel_initializer=\"normal\"))\n",
    "# Compile model\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "# Fit the model\n",
    "x = modified_carbon_data['Date_Num']\n",
    "y = modified_carbon_data['ppm']\n",
    "model.fit(x, y, epochs=500, batch_size=10,  verbose=2)\n",
    "predictions = model.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets plot the result of this fitting against the original data. Below the model's fit will be in blue against the original data in red:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the result\n",
    "plt.plot(x, y, 'ro', label ='Original data') \n",
    "plt.plot(x, predictions, label ='Fitted line') \n",
    "plt.title('Non Linear Regression Result') \n",
    "plt.legend() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's return to the sea ice data. Again, we will normalize and plot the data before passing it into Tensorflow. In Tensorflow we will train a sequential model that will attempt to predict future values northern sea ice extent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(sea_ice['Year_Month_Day'])\n",
    "climate_df['Date_Num'] = climate_df['Year_Month_Day'].map(dt.datetime.toordinal)\n",
    "\n",
    "#Data preprocessing\n",
    "modified_climate_data = climate_df[['Date_Num','ExtentNorth','ExtentSouth','LandAverageTemperature','LandMaxTemperature']] #temp df\n",
    "modified_climate_data = modified_climate_data.dropna()\n",
    "modified_climate_data = modified_climate_data.apply(lambda x : x/x.max(),axis=0)\n",
    "modified_climate_data.plot(kind='line',x='Date_Num',y='ExtentNorth',color='red')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Train model\n",
    "Now with our second modified dataset, we are again ready to train a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert an array of values into a dataset matrix\n",
    "def create_dataset(dataset, look_back=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-look_back-1):\n",
    "        a = dataset[i:(i+look_back)]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + look_back])\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)\n",
    "#create the data set from previously generated modified climate data\n",
    "dataframe = pd.concat([modified_climate_data.Date_Num, modified_climate_data.ExtentNorth],axis=1)\n",
    "dataframe = dataframe['ExtentNorth']\n",
    "dataset = dataframe.values\n",
    "dataset = dataset.astype(float)\n",
    "print(dataset)\n",
    "\n",
    "# split into train and test sets\n",
    "train_size = int(len(dataset) * 0.70)\n",
    "print(train_size)\n",
    "test_size = len(dataset) - train_size\n",
    "train, test = dataset[0:train_size], dataset[train_size:len(dataset)]\n",
    "\n",
    "# reshape into X=t and Y=t+1 for prediction\n",
    "look_back = 1\n",
    "trainX, trainY = create_dataset(train, look_back)\n",
    "testX, testY = create_dataset(test, look_back)\n",
    "\n",
    "print('Train and Test Data Set')\n",
    "plt.plot(trainX)\n",
    "plt.plot(testX)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# reshape input to be [samples, time steps, features]\n",
    "trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
    "\n",
    "\n",
    "\n",
    "# create and fit the LSTM network\n",
    "model = Sequential()\n",
    "model.add(LSTM(4, input_shape=(1, look_back)))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(trainX, trainY, epochs=10, batch_size=1, verbose=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can make some predictions and plot those predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "trainPredict = model.predict(trainX)\n",
    "testPredict = model.predict(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the data set\n",
    "trainPredictPlot = np.empty_like(dataset)\n",
    "trainPredictPlot[:] = np.nan\n",
    "trainPredictPlot[look_back:len(trainPredict)+look_back] = trainPredict[:,0]\n",
    "\n",
    "# shift test predictions for plotting\n",
    "testPredictPlot = np.empty_like(dataset)\n",
    "testPredictPlot[:] = np.nan\n",
    "testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1] = testPredict[:,0]\n",
    "# plot baseline and predictions\n",
    "plt.plot(dataset,'r')\n",
    "plt.plot(trainPredictPlot,'g')\n",
    "plt.plot(testPredictPlot,'b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Use model to predict value\n",
    "Now we have trained and fit our model. Let's try and predict the next value for northern sea ice extent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict the next value with the model\n",
    "data_x = [0.33, 0.55, 0.77]\n",
    "x,y = create_dataset(data_x,look_back) #reform the dataset for next sequence\n",
    "x_t = np.reshape(x,(x.shape[0],1,x.shape[1]))#reshape the data set to fit into the dl_model\n",
    "y_t = model.predict(x_t)\n",
    "print(x_t, ' has the predicted next value: ', y_t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
